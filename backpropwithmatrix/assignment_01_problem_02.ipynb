{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4 align=\"right\">by <a href=\"http://cse.iitkgp.ac.in/~adas/\">Abir Das</a> with help of <br> Ram Rakesh and Ankit Singh<br> </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the following details here\n",
    "** Name: ** `<shravan kumar singh>`<br/>\n",
    "** Roll Number: ** `<18cs92r07>`<br/>\n",
    "** Department: ** `<computer science and engineering>`<br/>\n",
    "** Email: ** `<shan.icwa@gmail.com>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "To run and solve this assignment, one must have a working IPython Notebook installation. The easiest way to set it up for both Windows and Linux is to install [Anaconda](https://www.continuum.io/downloads). Then save this file ([`assignment_01.ipynb`]()) to your computer, run Anaconda and choose this file in Anaconda's file explorer. Use `Python 3` version. Below statements assume that you have already followed these instructions. If you are new to Python or its scientific library, Numpy, there are some nice tutorials [here](https://www.learnpython.org/) and [here](http://www.scipy-lectures.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: You will implement a fully connected neural network from scratch in this problem\n",
    "We marked places where you are expected to add/change your own code with **`##### write your code below #####`** comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "597wDiAvGvuB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You are not supposed to import any other python library to work on this assignments.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "'''You are not supposed to import any other python library to work on this assignments.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "B54oZmm1DNWe",
    "outputId": "8c59bd48-230d-4fb9-eba1-82471de363df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 60000\n"
     ]
    }
   ],
   "source": [
    "'''data is loaded from data directory.\n",
    "please don't remove the folder '''\n",
    "\n",
    "x_train = np.load('./data/X_train.npy')\n",
    "x_train = x_train.flatten().reshape(-1,28*28)\n",
    "x_train = x_train / 255.0\n",
    "gt_indices = np.load('./data/y_train.npy')\n",
    "train_length = len(x_train)\n",
    "print(\"Number of training examples: {:d}\".format(train_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LvVFhXNB5xrD"
   },
   "outputs": [],
   "source": [
    "'''Dimensions to be used for creating your model'''\n",
    "\n",
    "batch_size = 64  # batch size\n",
    "input_dim = 784  # input dimension\n",
    "hidden_1_dim = 512  # hidden layer 1 dimension\n",
    "hidden_2_dim = 256  # hidden layer 2 dimension\n",
    "output_dim = 10   # output dimension\n",
    "\n",
    "'''Other hyperparameters'''\n",
    "learning_rate = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hImaaujc5zXg"
   },
   "outputs": [],
   "source": [
    "#creating one hot vector representation of output classification\n",
    "y_train = np.zeros((train_length, output_dim))\n",
    "# print(y.shape, gt_indices.shape)\n",
    "for i in range(train_length):\n",
    "    y_train[i,gt_indices[i]] = 1\n",
    "\n",
    "# Number of mini-batches (as integer) in one epoch\n",
    "num_minibatches = np.floor(train_length/batch_size).astype(int) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "W7lHWEWVaVlK",
    "outputId": "4ecb1bfc-4568-44cb-e109-57677da50eb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of mini-batches 937 and total training data used in training:59968.\n"
     ]
    }
   ],
   "source": [
    "print(\"No of mini-batches {:d} and total training data used in training:\\\n",
    "{}.\".format(num_minibatches, num_minibatches*batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9HRf0Wj52cK"
   },
   "outputs": [],
   "source": [
    "'''Randomly Initialize Weights  from standard normal distribution (i.e., mean = 0 and s.d. = 1.0).\n",
    "Use the dimesnions specified in the cell 3 to initialize your weights matrices. \n",
    "Use the nomenclature W1,W2 etc. (provided below) for the different weight matrices.'''\n",
    "\n",
    "########################## write your code below ##############################################\n",
    "W1 = np.random.normal(0,1.0,(input_dim,hidden_1_dim))\n",
    "W2 = np.random.normal(0,1.0,(hidden_1_dim,hidden_2_dim))\n",
    "W3 = np.random.normal(0,1.0,(hidden_2_dim,output_dim))\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmZRrEVb6CJy"
   },
   "outputs": [],
   "source": [
    "# Write a function which computes the softmax where X is vector of scores computed during forward pass\n",
    "def softmax(x):\n",
    "    ##############################write your code here #################################\n",
    "    maxi=np.max(x,axis=1)\n",
    "    maxi=maxi.reshape((x.shape[0], 1))\n",
    "    x=x - maxi#np.expand_dims(np.max(expx, axis = axis), axis)\n",
    "    #print(maxi)\n",
    "    expx=np.exp(x)    \n",
    "    sumx=np.sum(expx,axis=1)    \n",
    "    sumx=sumx.reshape((x.shape[0],1))\n",
    "    #print(sumx)    \n",
    "    #print(np.sum(expx/sumx,axis=1))\n",
    "    return expx/sumx   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "Gjz4yhwE6JQw",
    "outputId": "341578db-29a4-48ca-b0f8-a0343aadd24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch: 0, iteration: 0, Loss: 14.6790 \n",
      " Epoch: 1, iteration: 937, Loss: 11.2251 \n",
      " Epoch: 2, iteration: 1874, Loss: 9.0129 \n",
      " Epoch: 3, iteration: 2811, Loss: 7.7712 \n",
      " Epoch: 4, iteration: 3748, Loss: 7.4513 \n",
      " Epoch: 5, iteration: 4685, Loss: 7.0338 \n",
      " Epoch: 6, iteration: 5622, Loss: 6.7759 \n",
      " Epoch: 7, iteration: 6559, Loss: 6.0135 \n",
      " Epoch: 8, iteration: 7496, Loss: 5.7523 \n",
      " Epoch: 9, iteration: 8433, Loss: 5.4685 \n",
      " Epoch: 10, iteration: 9370, Loss: 5.4686 \n",
      " Epoch: 11, iteration: 10307, Loss: 5.4686 \n",
      " Epoch: 12, iteration: 11244, Loss: 5.4686 \n",
      " Epoch: 13, iteration: 12181, Loss: 5.4686 \n",
      " Epoch: 14, iteration: 13118, Loss: 5.4686 \n",
      " Epoch: 15, iteration: 14055, Loss: 5.4686 \n",
      " Epoch: 16, iteration: 14992, Loss: 5.4686 \n",
      " Epoch: 17, iteration: 15929, Loss: 5.4686 \n",
      " Epoch: 18, iteration: 16866, Loss: 5.4686 \n",
      " Epoch: 19, iteration: 17803, Loss: 5.4685 \n",
      " Epoch: 20, iteration: 18740, Loss: 5.4685 \n",
      " Epoch: 21, iteration: 19677, Loss: 5.4684 \n",
      " Epoch: 22, iteration: 20614, Loss: 5.4684 \n",
      " Epoch: 23, iteration: 21551, Loss: 5.4683 \n",
      " Epoch: 24, iteration: 22488, Loss: 5.4682 \n",
      " Epoch: 25, iteration: 23425, Loss: 5.4681 \n",
      " Epoch: 26, iteration: 24362, Loss: 5.4680 \n",
      " Epoch: 27, iteration: 25299, Loss: 5.4678 \n",
      " Epoch: 28, iteration: 26236, Loss: 5.4677 \n",
      " Epoch: 29, iteration: 27173, Loss: 5.4674 \n",
      " Epoch: 30, iteration: 28110, Loss: 5.4674 \n",
      " Epoch: 31, iteration: 29047, Loss: 5.4674 \n",
      " Epoch: 32, iteration: 29984, Loss: 5.4674 \n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xl0HOWd7vHnp91avMiS5RXLliyzryJgWwYyBA4hEEhCCGAyBAgEyJ1sZBIymXsnw9xkcrNPhm1I2CYBMoQlYYDkQli9sMnGLDHYlhe8YFuyjWx5ka3lN390eSI8WlpSd7/d0vdzjo6t6lLXU3Xa4qHqrXrN3QUAAIDUygodAAAAYDiihAEAAARACQMAAAiAEgYAABAAJQwAACAAShgAAEAAlDAACMTMTjOzDaFzAAiDEgYgYcxsrZl9JHQOAMgElDAAAIAAKGEAUsLMrjKzBjPbbmaPmtnEaLmZ2U/NrNHMdpjZG2Z2ZPTa2Wa2zMxazGyjmX29m/fNN7PmAz8TLSs3s71mNs7MyszssWid7WY238y6/d1nZoea2VPResvN7MIur91tZrdFr7eY2fNmNrXL67PN7NVoH141s9ldXis1s7vM7D0ze9/MfnfQdq+P9n+TmV3eZXmf+w8gc1HCACSdmf2VpH+WdKGkCZLelfSb6OUzJZ0iqUbSaEmfkbQteu0OSV9w9xJJR0p65uD3dvd9kh6WdHGXxRdKet7dGyVdL2mDpHJJFZL+TtL/mK/NzIokPSXpPknjove7xcyO6LLaPEn/JKlM0lJJ90Y/WyrpcUk/lzRW0k8kPW5mY6Of+5WkQklHRO/90y7vOV7SKEmTJF0p6WYzGxPv/gPIXJQwAKkwT9Kd7r4kKk3fkjTLzColtUkqkXSoJHP3t919U/RzbZION7OR7v6+uy/p4f3v0wdL2CXRsgPvMUHSVHdvc/f53v2kuedIWuvud7l7e7SthyRd0GWdx939hWgfvh3twxRJH5O00t1/Ff3s/ZLekXSumU2Q9FFJ10T70Obuz3d5zzZJN0bLn5C0S9LMfu4/gAxECQOQChMVO/slSXL3XYqd7Zrk7s9IuknSzZK2mNntZjYyWvVTks6W9G50+W9WD+//jKQRZnZSdInwWEmPRK/9UFKDpCfNbLWZ3dDDe0yVdFJ02bLZzJoVK4/ju6yz/qB92B7t2wf2L/KuYme3pkja7u7v97Ddbe7e3uX7PZKK+7n/ADIQJQxAKrynWMmR9N+X/sZK2ihJ7v5zdz9Bsct1NZL+Nlr+qrufp9glvN9JeqC7N3f3zui1ixU7C/aYu7dEr7W4+/XuPl3SuZK+Zmand/M26xW7hDm6y1exu1/bZZ0pXfahWFJptG8f2L/IIdH+rZdUamaj+zpI3exXXPsPIDNRwgAkWq6ZFXT5ylHs0uDlZnasmeVL+p6kl919rZmdGJ3BypW0W1KrpA4zyzOzeWY2yt3bJO2U1NHLdu9TbDzZPP3lUqTM7BwzqzYz6/Ie3b3PY5JqzOyzZpYbfZ1oZod1WedsM6szszzFxoa97O7rJT0R/ewlZpZjZp+RdLhiZXCTpD8oNr5sTPS+p/R1EAew/wAyDCUMQKI9IWlvl6/vuPvTkv63YmOsNkmqknRRtP5ISb+Q9L5il/C2SfpR9NpnJa01s52SrpF0aU8bdfeXFStxExUrPQfMkPQnxcZavSjpFnd/rpufb1HsJoGLFDuztVnS/5OU32W1+yT9g2KXIU9QrPDJ3bcpNqbs+ij/NySd4+5bu+xHm2LjxBolfaWn/ThI3PsPIPNY9+NTAQBdmdndkja4+9+HzgJgaOBMGAAAQACUMAAAgAC4HAkAABAAZ8IAAAACoIQBAAAEkBM6QDzKysq8srIydAwAAIA+LV68eKu7l/e1XkaUsMrKStXX14eOAQAA0CczO3gas25xORIAACAAShgAAEAAlDAAAIAAKGEAAAABUMIAAAACoIQBAAAEQAkDAAAIgBIGAAAQACUMAAAgAEqYpJVbWnT3wjVy99BRAADAMEEJk7Ro1TZ95z+XqallX+goAABgmKCESZpRUSxJWr6lJXASAAAwXFDCJM2sKJEkLd9MCQMAAKlBCZM0tjhfZcV5WrllV+goAABgmKCERWoqSrgcCQAAUoYSFqmpKNHKLS3q7OQOSQAAkHyUsEhNRYl27+/Qxua9oaMAAIBhgBIWmTk+dofkCi5JAgCAFKCERWZEd0iuYHA+AABIAUpYZGRBriaOKuBMGAAASAlKWBc140t4VhgAAEgJSlgXNRUlamjapfaOztBRAADAEEcJ66KmokT72zv17vY9oaMAAIAhjhLWxYHpi1YyLgwAACQZJayL6nHFMpOWb+YOSQAAkFxJK2FmdqeZNZrZW9289nUzczMrS9b2B2JEXramlhZyhyQAAEi6ZJ4Ju1vSWQcvNLMpks6QtC6J2x6wGcwhCQAAUiBpJczdX5C0vZuXfirpG5LScpLGmRUlWrN1t/a1d4SOAgAAhrCUjgkzs49L2ujur6dyu/1RM75EHZ2uNVt3h44CAACGsJSVMDMrlPRtSf8nzvWvNrN6M6tvampKbrguDtwhyUNbAQBAMqXyTFiVpGmSXjeztZImS1piZuO7W9ndb3f3WnevLS8vT1nIaWVFyskyBucDAICkyknVhtz9TUnjDnwfFbFad9+aqgzxyMvJ0rSyIh5TAQAAkiqZj6i4X9KLkmaa2QYzuzJZ20q0mvElnAkDAABJlbQzYe5+cR+vVyZr24M1s6JET7y5SXv2t6swL2UnCwEAwDDCE/O7UVNRInepoZFLkgAAIDkoYd2oqSiWxB2SAAAgeShh3Zg6tkh5OVmMCwMAAElDCetGdpZpxrhiLd/C5UgAAJAclLAezKwo0UrOhAEAgCShhPWgZnyJNu1o1Y69baGjAACAIYgS1oMDg/M5GwYAAJKBEtaDmgNzSFLCAABAElDCejBp9AgV5WVrBY+pAAAASUAJ64GZRdMXcYckAABIPEpYL2ZWMIckAABIDkpYL2ZUlGjb7v3aumtf6CgAAGCIoYT1YmY0OJ9xYQAAINEoYb2oGR/NIcklSQAAkGCUsF6UF+drTGEug/MBAEDCUcJ6YWaqYXA+AABIAkpYH2oqSrRic4vcPXQUAAAwhFDC+lAzvkQt+9q1aUdr6CgAAGAIoYT1YSbTFwEAgCSghPWBibwBAEAyUML6MLowTxUj87V8M3dIAgCAxKGExYE7JAEAQKJRwuJQU1GilY0t6ujkDkkAAJAYlLA4zKwoUWtbp9Zv3xM6CgAAGCIoYXGoGR/NIcklSQAAkCCUsDjMGBe7Q5ISBgAAEoUSFoei/BxNHjNCy5lDEgAAJAglLE4zo+mLAAAAEoESFqea8SVavXWX2jo6Q0cBAABDACUsTjMrStTW4Vq7dXfoKAAAYAighMWphjkkAQBAAlHC4jS9vEhZJsaFAQCAhKCExakgN1uVZUWcCQMAAAlBCeuHmRUlWsFjKgAAQAJQwvqhpqJE727brda2jtBRAABAhqOE9cPM8SXqdKmhkbNhAABgcChh/VBTwfRFAAAgMShh/TB1bJHysrMYnA8AAAaNEtYPudlZml5exGMqAADAoFHC+mnmeO6QBAAAg0cJ66eaihJtbN6rlta20FEAAEAGo4T104Hpi1ZyhyQAABgESlg/zYxKGOPCAADAYFDC+mnymBEakZvNHZIAAGBQklbCzOxOM2s0s7e6LPuhmb1jZm+Y2SNmNjpZ20+WrCxTTUWxVjI4HwAADEIyz4TdLemsg5Y9JelIdz9a0gpJ30ri9pOmpqKEM2EAAGBQklbC3P0FSdsPWvaku7dH374kaXKytp9MNRUlamrZp+2794eOAgAAMlTIMWFXSPpDwO0PWM34aHA+Z8MAAMAABSlhZvZtSe2S7u1lnavNrN7M6puamlIXLg7/fYckJQwAAAxQykuYmV0m6RxJ89zde1rP3W9391p3ry0vL09dwDhUjMzXyIIcShgAABiwnFRuzMzOkvRNSae6+55UbjuRzCw2fdFm7pAEAAADk8xHVNwv6UVJM81sg5ldKekmSSWSnjKzpWZ2W7K2n2wzojskezmZBwAA0KOknQlz94u7WXxHsraXajMrSnTf3nVqbNmnipEFoeMAAIAMwxPzB+jAHJLLmb4IAAAMACVsgGoqiiVxhyQAABgYStgAjS3OV1lxPiUMAAAMCCVsEGoqirWcOSQBAMAAUMIGoaaiRCu3tKizkzskAQBA/1DCBmHm+BLt2d+hjc17Q0cBAAAZhhI2CDVMXwQAAAaIEjYIM6I7JJdTwgAAQD9RwgZhZEGuJo4q0AqeFQYAAPqJEjZINeNLuEMSAAD0GyVskGZWlGhV4y61d3SGjgIAADIIJWyQaipKtL+jU2u37QkdBQAAZBBK2CAduENyJYPzAQBAP1DCBql6XLHMuEMSAAD0DyVskEbkZWtqaSHPCgMAAP1CCUuAmooSvcNjKgAAQD9QwhLgmCmjtbppt7bt2hc6CgAAyBCUsASYU10mSVq0alvgJAAAIFNQwhLgqEmjVFKQowUrt4aOAgAAMgQlLAGys0yzq8ZqQcNWuXvoOAAAIANQwhKkbka5Njbv1bs8tBUAAMSBEpYgddG4sPkNXJIEAAB9o4QlSOXYQk0aPUILGRcGAADiQAlLEDNTXXWZFq3aqo5OxoUBAIDeUcISaM6MMu1sbdebG3eEjgIAANIcJSyB5lSNlSQtZFwYAADoAyUsgcYW5+vwCSM1f2VT6CgAACDNUcISrG5GmZa826w9+9tDRwEAAGmMEpZgddVl2t/RqVfWbA8dBQAApDFKWIKdWFmqvOwsxoUBAIBeUcISbERetmorx2hBA5N5AwCAnlHCkmBOdZne3rRTTS37QkcBAABpihKWBAemMFq0ikuSAACge5SwJDhy0iiNGpGrBUxhBAAAekAJS4LsLNPsqrFa2LBV7kxhBAAA/idKWJLUzSjTeztatWbr7tBRAABAGqKEJcmBcWELeFQFAADoBiUsSQ4pLdTkMSMYFwYAALoVVwkzsyozy4/+fpqZfcnMRic3WmYzM82dUaYXV21Te0dn6DgAACDNxHsm7CFJHWZWLekOSdMk3Ze0VEPEnOoytexr1xsbd4SOAgAA0ky8JazT3dslfULSz9z9q5ImJC/W0DC7qkxm0kIuSQIAgIPEW8LazOxiSZdJeixalpucSENHaVGejpg4UvMZnA8AAA4Sbwm7XNIsSd919zVmNk3Sr5MXa+iYU12m19a9r9372kNHAQAAaSSuEubuy9z9S+5+v5mNkVTi7t9PcrYhYW51udo6XK+s2R46CgAASCPx3h35nJmNNLNSSa9LusvMfpLcaENDbeUY5eVk8bwwAADwAfFejhzl7jslfVLSXe5+gqSP9PYDZnanmTWa2VtdlpWa2VNmtjL6c8zAo2eGgtxsnVg5hueFAQCAD4i3hOWY2QRJF+ovA/P7creksw5adoOkp919hqSno++HvLrqci3f0qLGltbQUQAAQJqIt4TdKOn/S1rl7q+a2XRJK3v7AXd/QdLBA6HOk3RP9Pd7JJ3fj6wZ68AURosatgVOAgAA0kW8A/N/6+5Hu/u10fer3f1TA9hehbtvit5jk6RxA3iPjHPExJEaXZir+VySBAAAkXgH5k82s0eiMV5bzOwhM5uczGBmdrWZ1ZtZfVNTUzI3lXRZWaY5VWVa2LBV7h46DgAASAPxXo68S9KjkiZKmiTpP6Nl/bUlGlum6M/GnlZ099vdvdbda8vLywewqfQyp7pMm3e2alXTrtBRAABAGoi3hJW7+13u3h593S1pIM3oUcWeuq/oz98P4D0y0twZsXFh3CUJAACk+EvYVjO71Myyo69LJfU6ytzM7pf0oqSZZrbBzK6U9H1JZ5jZSklnRN8PC1NKC3VIaaEWMDgfAABIyolzvSsk3STpp5Jc0iLFpjLqkbtf3MNLp8edboipm1GmR5e+p7aOTuVmx9t/AQDAUBTv3ZHr3P3j7l7u7uPc/XzFHtyKfqirLtOufe16Y0Nz6CgAACCwwZyO+VrCUgwTs6aPlZl4VAUAABhUCbOEpRgmxhTl6ahJo7SQeSQBABj2BlPCeODVAMypLtNr65q1a1976CgAACCgXkuYmbWY2c5uvloUe2YY+mludZnaO10vr+YuSQAAhrNeS5i7l7j7yG6+Stw93jsr0cXxU8coPydLC7gkCQDAsMZzElKsIDdbH5pWykNbAQAY5ihhAdRVl2ll4y5t2dkaOgoAAAiEEhbAnOrYFEbcJQkAwPBFCQvg8AkjVVqUxyVJAACGMUpYAFlZptlVY7WgYavcedIHAADDESUskLrqMjW27NPKxl2howAAgAAoYYHUzYiNC+OSJAAAwxMlLJDJYwpVObaQwfkAAAxTlLCA5lSX6aXV29TW0Rk6CgAASDFKWEBzZ5Rp9/4OLV3fHDoKAABIMUpYQLOmlynLpPmMCwMAYNihhAU0qjBXR00ezbgwAACGIUpYYHXVY7V0fbNaWttCRwEAAClECQtsTnWZOjpdL63eHjoKAABIIUpYYCdMHaOC3CwuSQIAMMxQwgLLz8nW7KoyPf7mJrW2dYSOAwAAUoQSlgaurJumppZ9emjJhtBRAABAilDC0sDsqrE6Zspo3fb8KrXz4FYAAIYFSlgaMDNdd1qV1m/fq8ff3BQ6DgAASAFKWJo447AKzRhXrFueXaXOTg8dBwAAJBklLE1kZZmu+3CVlm9p0dPvNIaOAwAAkowSlkbOPXqiJo8ZoZuebZA7Z8MAABjKKGFpJCc7S9ecWqXX1zfrxVXbQscBAABJRAlLMxecMFnlJfm65blVoaMAAIAkooSlmYLcbH2+bpoWNGzV0vXNoeMAAIAkoYSloXknT9XIghzd8mxD6CgAACBJKGFpqDg/R5+bM01PLtuiFVtaQscBAABJQAlLU5fPrlRhXrZuZWwYAABDEiUsTY0pytMlHzpEj77+ntZv3xM6DgAASDBKWBr7/NzpyjbTv73A2TAAAIYaSlgaGz+qQJ86YZIeqN+gxp2toeMAAIAEooSluS+cUqX2jk7dsWBN6CgAACCBKGFprrKsSOccPVG/fuldNe/ZHzoOAABIEEpYBrj2tCrt3t+hexa9GzoKAABIEEpYBjhswkh95LBxumvRGu3e1x46DgAASABKWIa47sPVat7TpvtfWRc6CgAASABKWIY4/pAxOnl6qX4xf7X2tXeEjgMAAAaJEpZBvvjham3ZuU8PL9kYOgoAABgkSlgGqasu09GTR+m251epvaMzdBwAADAIQUqYmX3VzP5sZm+Z2f1mVhAiR6YxM113WrXe3bZHT7y1OXQcAAAwCCkvYWY2SdKXJNW6+5GSsiVdlOocmerMwytUPa5YtzzbIHcPHQcAAAxQqMuROZJGmFmOpEJJ7wXKkXGyskzXnlqldza36Jl3GkPHAQAAA5TyEubuGyX9SNI6SZsk7XD3J1OdI5N9/NiJmjR6hG7ibBgAABkrxOXIMZLOkzRN0kRJRWZ2aTfrXW1m9WZW39TUlOqYaS03O0vXnDpdr61r1kurt4eOAwAABiDE5ciPSFrj7k3u3ibpYUmzD17J3W9391p3ry0vL095yHT36dopKivO1y3PNYSOAgAABiBECVsn6WQzKzQzk3S6pLcD5MhoBbnZ+vzcaZq/cqve2NAcOg4AAOinEGPCXpb0oKQlkt6MMtye6hxDwbyTDtHIghzd8uyq0FEAAEA/Bbk70t3/wd0Pdfcj3f2z7r4vRI5MV1KQq8tmV+qPf96slVtaQscBAAD9wBPzM9zlc6ZpRG62rvr3ev3hzU3cLQkAQIaghGW40qI83XFZrfJysnTtvUv0iVsW6eXV20LHAgAAfaCEDQGzq8v0hy+foh986mht3tGqz9z+kq68+1Wt4BIlAABpyzLh8lVtba3X19eHjpERWts6dNfCtbrluQbt3teuTx0/WV87s0YTRo0IHQ0AgGHBzBa7e22f61HChqbmPft187MNumfRuzKTPjenUtedWq1RhbmhowEAMKRRwiBJ2vD+Hv3kqRV65LWNGlmQqy9+uEp/PatSBbnZoaMBADAkxVvCGBM2xE0eU6ifXHisHv+buTrukNH63hPv6PQfP6+HFm9QR2f6F3AAAIYqStgwcfjEkbr78g/pvqtO0tjiPF3/29f1sZ/P17PLG3msBQAAAVDChpnZVWX6/Rfn6KZLjtPetg5dfteruvgXL2n5Zu6kBAAglShhw5CZ6ZyjJ+qpr56qG887Qiu27NK5Ny3QnQvWqJNLlAAApAQlbBjLy8nSX8+q1JNfPUWnzCjTjY8t02V3vaItO1tDRwMAYMijhEFlxfn6xV/X6rufOFKvrt2us372gv741ubQsQAAGNIoYZAUu0Q576SpevxLczV5TKGu+fVifePB17V7X3voaAAADEmUMHxAVXmxHrp2tq47rUq/XbxBZ/98vl5b937oWAAADDmUMPwPeTlZ+sZZh+o3V52s9g7XBbe9qH/500q1d3SGjgYAwJBBCUOPTpo+Vk98ea7OPXqCfvqnFfrM7S9p3bY9oWMBADAkUMLQq1EjcvWzi47Tv1x0rFZsadHZP5+vBxdv4AGvAAAMEiUMcTnv2En6w5fn6vCJI/X1376uL963RM179oeOBQBAxqKEIW6TxxTq/qtO1jfOmqkn/7xFZ/1svhY2bA0dCwCAjEQJQ79kZ5muO61aj1w3R4X52Zr3y5f13ceXaRePsgAAoF8oYRiQoyaP0uN/M1fzTjpEv5i/RrO+97T+6bFlWr+dgfsAAMTDMmGAdW1trdfX14eOgR4sXd+sOxes0RNvblKnu844vEJXzJmmD00rlZmFjgcAQEqZ2WJ3r+1zPUoYEmXzjlb96qW1uvfldWre06bDJ4zUFXXTdO4xE5Sfkx06HgAAKUEJQzB793fod0s36s4Fa7SycZfKivN06clTNe+kqSovyQ8dDwCApKKEITh318KGbbpz4Ro9806j8rKzdO4xE3VFXaWOmDgqdDwAAJIi3hKWk4owGJ7MTHUzylQ3o0yrm3bp7kVr9eDiDXpoyQadNK1UV9RN00cOq1B2FuPGAADDD2fCkFI79rbpgVfX6+5Fa7Wxea+mlI7QFXOm6bJZlcqijAEAhoB4z4TxiAqk1KgRubrqlOl6/m9P063zjtf4kQX6x/9cpn99piF0NAAAUooShiBysrP00aMm6IEvzNInj5+knz29Qs8tbwwdCwCAlKGEISgz03fPP0ozK0r0lf9Yqg3v87BXAMDwQAlDcCPysnXbpSeoo8N13b1L1NrWEToSAABJRwlDWqgsK9KPLzxGb2zYoRsfWxY6DgAASUcJQ9o484jxuva0Kt338jo9uHhD6DgAACQVJQxp5fozajRr+lh9+5E3tey9naHjAACQNJQwpJWc7Cz96yXHaXRhrq69d7F27G0LHQkAgKSghCHtlBXn65Z5x2vj+3t1/QNL1dmZ/g8UBgCgvyhhSEsnTC3V33/sMP3p7Ubd+vyq0HEAAEg4ShjS1mWzK/XxYybqx08u18KGraHjAACQUJQwpC0z0z9/8ihVlRfrS/e/pk079oaOBABAwlDCkNaK8nN066UnqLWtQ9fdu0T72ztDRwIAICEoYUh71eOK9cNPH6PX1jXre0+8HToOAAAJQQlDRjj7qAn6fN003b1orX6/dGPoOAAADBolDBnjmx89VCdWjtEND72pFVtaQscBAGBQKGHIGLnZWbr5kuNVlJ+ja361WC2tPMgVAJC5gpQwMxttZg+a2Ttm9raZzQqRA5ln3MgC3XzJcXp3+x5948E35M6DXAEAmSnUmbB/kfRHdz9U0jGSGG2NuJ00faxuOOtQ/eGtzfrl/DWh4wAAMCApL2FmNlLSKZLukCR33+/uzanOgcz2+bnT9NEjx+v7f3xHL6/eFjoOAAD9FuJM2HRJTZLuMrPXzOyXZlYUIAcymJnpBxccramlhbr6V4v186dXatuufaFjAQAQtxAlLEfS8ZJudffjJO2WdMPBK5nZ1WZWb2b1TU1Nqc6IDFBSkKs7PneijjtktH7y1ArN/v4z+tbDb2gld04CADKApXpgs5mNl/SSu1dG38+VdIO7f6ynn6mtrfX6+voUJUQmamhs0R0L1urhJRu0r71Tp9SU68q6aTplRpnMLHQ8AMAwYmaL3b22r/VSfibM3TdLWm9mM6NFp0taluocGFqqx5Xonz95lF781un6+pk1envTTl125ys686cv6P5X1qm1rSN0RAAAPiDlZ8IkycyOlfRLSXmSVku63N3f72l9zoShv/a1d+ix1zfpjgVrtGzTTpUW5enSkw7RpbOmalxJQeh4AIAhLN4zYUFKWH9RwjBQ7q6XVm/XHQvW6Ol3tig3K0vnHjNRV9ZN0+ETR4aOBwAYguItYTmpCAOEYmaaVTVWs6rGas3W3bpr4Rr9tn6DHlqyQbOmj9WVddP0V4eOU1YW48YAAKnFmTAMOzv2tOn+V9fpnkVrtWlHq6aXF+naU6t0/nGTlJvNTF4AgMHhciTQh7aOTj3x5ibd9vxqvb1ppyaNHqFrTp2uT9dOUUFuduh4AIAMRQkD4uTuenZ5o256pkFL1jWrvCRfV82dpktOmqrifK7YAwD6hxIG9JO768XV23Tzsw1a2LBNo0bk6vI5lfrc7EqNLswLHQ8AkCEoYcAgvLbufd38bIP+9HajivKydemsqfp83XSVl+SHjgYASHOUMCAB3t60U7c8t0qPv/GecrOzdNGJU3T1qVWaNHpE6GgAgDRFCQMSaM3W3br1uQY9vGSjJOmTx0/StadVa1oZc88DAD6IEgYkwcbmvbr9+VX6zavr1dbRqY8dPVFnHF4hnjKGEEJPi2oBP/nh932QPz/o/IN7g76239e7Z/KcvEdNGqXxo5I7cwolDEiippZ9umPBGv3qxbXavZ95KQEgU9x0yXE65+iJSd0GJQxIgZbWNm3Z2Ro6Boah0L+6Q24+/L4PLsBg8w/65/vIH/r4JtuUMYUaVZib1G0wbRGQAiUFuSopSO4/ZgDA0MQcLQAAAAFQwgAAAAL4LwQ2AAAGXUlEQVSghAEAAARACQMAAAiAEgYAABAAJQwAACAAShgAAEAAlDAAAIAAKGEAAAABUMIAAAACyIi5I82sSdK7Sd5MmaStSd7GUMbxGziO3cBx7AaOYzc4HL+BGw7Hbqq7l/e1UkaUsFQws/p4JttE9zh+A8exGziO3cBx7AaH4zdwHLu/4HIkAABAAJQwAACAAChhf3F76AAZjuM3cBy7gePYDRzHbnA4fgPHsYswJgwAACAAzoQBAAAEQAmTZGZnmdlyM2swsxtC58kkZrbWzN40s6VmVh86T7ozszvNrNHM3uqyrNTMnjKzldGfY0JmTFc9HLvvmNnG6PO31MzODpkxXZnZFDN71szeNrM/m9mXo+V89vrQy7Hjs9cHMysws1fM7PXo2P1jtHyamb0cfe7+w8zyQmcNZdhfjjSzbEkrJJ0haYOkVyVd7O7LggbLEGa2VlKtuw/1Z74khJmdImmXpH939yOjZT+QtN3dvx/9T8AYd/9myJzpqIdj9x1Ju9z9RyGzpTszmyBpgrsvMbMSSYslnS/pc+Kz16tejt2F4rPXKzMzSUXuvsvMciUtkPRlSV+T9LC7/8bMbpP0urvfGjJrKJwJkz4kqcHdV7v7fkm/kXRe4EwYotz9BUnbD1p8nqR7or/fo9gveBykh2OHOLj7JndfEv29RdLbkiaJz16fejl26IPH7Iq+zY2+XNJfSXowWj6sP3eUsNg/pvVdvt8g/oH1h0t60swWm9nVocNkqAp33yTFfuFLGhc4T6b5X2b2RnS5kstpfTCzSknHSXpZfPb65aBjJ/HZ65OZZZvZUkmNkp6StEpSs7u3R6sM6//mUsIk62bZ8L5G2z9z3P14SR+V9MXokhGQKrdKqpJ0rKRNkn4cNk56M7NiSQ9J+oq77wydJ5N0c+z47MXB3Tvc/VhJkxW78nRYd6ulNlX6oITFWviULt9PlvReoCwZx93fi/5slPSIYv/I0D9bonEnB8afNAbOkzHcfUv0S75T0i/E569H0ZichyTd6+4PR4v57MWhu2PHZ69/3L1Z0nOSTpY02sxyopeG9X9zKWGxgfgzors18iRdJOnRwJkygpkVRQNVZWZFks6U9FbvP4VuPCrpsujvl0n6fcAsGeVAgYh8Qnz+uhUNkL5D0tvu/pMuL/HZ60NPx47PXt/MrNzMRkd/HyHpI4qNqXtW0gXRasP6czfs746UpOjW4p9JypZ0p7t/N3CkjGBm0xU7+yVJOZLu49j1zszul3SapDJJWyT9g6TfSXpA0iGS1kn6tLszAP0gPRy70xS7HOSS1kr6woExTvgLM6uTNF/Sm5I6o8V/p9jYJj57vejl2F0sPnu9MrOjFRt4n63YSZ8H3P3G6L8dv5FUKuk1SZe6+75wScOhhAEAAATA5UgAAIAAKGEAAAABUMIAAAACoIQBAAAEQAkDAAAIgBIGAAcxs9PM7LHQOQAMbZQwAACAAChhADKWmV1qZq+Y2VIz+7dosuBdZvZjM1tiZk+bWXm07rFm9lI04fIjByZcNrNqM/uTmb0e/UxV9PbFZvagmb1jZvdGT06XmX3fzJZF7/OjQLsOYAighAHISGZ2mKTPKDaJ/LGSOiTNk1QkaUk0sfzzij1ZX5L+XdI33f1oxZ5+fmD5vZJudvdjJM1WbDJmSTpO0lckHS5puqQ5Zlaq2BQ1R0Tv83+Tu5cAhjJKGIBMdbqkEyS9amZLo++nKza1zH9E6/xaUp2ZjZI02t2fj5bfI+mUaO7TSe7+iCS5e6u774nWecXdN0QTNC+VVClpp6RWSb80s09KOrAuAPQbJQxApjJJ97j7sdHXTHf/Tjfr9TY3m/XyWte57Dok5bh7u6QPSXpI0vmS/tjPzADw3yhhADLV05IuMLNxkmRmpWY2VbHfaxdE61wiaYG775D0vpnNjZZ/VtLz7r5T0gYzOz96j3wzK+xpg2ZWLGmUuz+h2KXKY5OxYwCGh5zQAQBgINx9mZn9vaQnzSxLUpukL0raLekIM1ssaYdi48Yk6TJJt0Ula7Wky6Pln5X0b2Z2Y/Qen+5lsyWSfm9mBYqdRftqgncLwDBi7r2dqQeAzGJmu9y9OHQOAOgLlyMBAAAC4EwYAABAAJwJAwAACIASBgAAEAAlDAAAIABKGAAAQACUMAAAgAAoYQAAAAH8F7NJKhjjMVbyAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "no_of_iterations = 30000\n",
    "loss_list=[]\n",
    "i_epoch = 0\n",
    "for i_iter in range(no_of_iterations):\n",
    "    \n",
    "    ''''''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size]\n",
    "    #print(x_batchinput.shape)\n",
    "    ########################## write your code below ##############################################\n",
    "    ######################### Forward Pass Block #####################################\n",
    "    '''Write the code for forward block of the neural network with 2 hidden layers.\n",
    "    Please stick to the notation below which follows the notation provided in the lecture slides.\n",
    "    Note that you are allowed to write the right hand sides of these variables in more than\n",
    "    one line if that is convenient for you.'''\n",
    "    \n",
    "    a1 =np.dot(x_batchinput,W1) # (64*784)*(784 * 512)\n",
    "    # implement Relu layer\n",
    "    h1 = np.where(a1>0,a1,0) # 512*64\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(h1,W2) #64*512 512*256# (512*256)' * 512*64 \n",
    "    # implement Relu activation \n",
    "    h2 = np.where(a2>0,a2,0) # 256*64\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2,W3) #64*256 256*10#(256*10)' * 256*64\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) # 64*10 #10*64\n",
    "    #enusre you have implemented the softmax function defined above\n",
    "    ##################################################################################\n",
    "    ###############################################################################################\n",
    "\n",
    "    neg_log_softmax_score = -np.log(softmax_score+0.00000001) # The small number is added to avoid 0 input to log function\n",
    "    #print(neg_log_softmax_score)\n",
    "    # Compute and print loss\n",
    "    if i_iter%num_minibatches == 0:\n",
    "        loss = np.mean(np.diag(np.take(neg_log_softmax_score, gt_indices[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size],\\\n",
    "                                       axis=1)))\n",
    "        print(\" Epoch: {:d}, iteration: {:d}, Loss: {:6.4f} \".format(i_epoch, i_iter, loss))\n",
    "        loss_list.append(loss)\n",
    "        i_epoch += 1\n",
    "        # Each 10th epoch reduce learning rate by a factor of 10\n",
    "        if i_epoch%10 == 0:\n",
    "            learning_rate /= 10.0\n",
    "    \n",
    "    ################################### Backpropagation Code Block #####################################\n",
    "    ''' Use the convention grad_{} for computing the gradients.\n",
    "    for e.g \n",
    "        grad_W1 for gradients w.r.t. weight W1\n",
    "        grad_w2 for gradients w.r.t. weights W2'''\n",
    "    ########################## write your code below ##############################################\n",
    "    # Gradient of cross-entropy loss w.r.t. preactivation of the output layer\n",
    "    grad_softmax_score = softmax_score-y_train[batch_elem_idx*batch_size:(batch_elem_idx+1)*batch_size] #64*10 #10*64\n",
    "    \n",
    "    # gradient w.r.t W3\n",
    "    grad_W3 = 1/batch_size*(h2.T).dot(grad_softmax_score) #(64*256)' 64*10\n",
    "    # gradient w.r.t h2\n",
    "    grad_h2 = np.where(h2>0,1,0)\n",
    "    # gradient w.r.t a2\n",
    "    grad_a2 = np.multiply(grad_softmax_score.dot(W3.T) ,grad_h2)\n",
    "    # gradient w.r.t W2\n",
    "    grad_W2 = 1/batch_size*(h1.T).dot(grad_a2)\n",
    "    # gradient w.r.t h1\n",
    "    grad_h1 = np.where(h1>0,1,0)\n",
    "    # gradient w.r.t a1\n",
    "    grad_a1 = np.multiply(grad_a2.dot(W2.T) ,grad_h1)\n",
    "    # gradient w.r.t W1\n",
    "    grad_W1 = 1/batch_size*(x_batchinput.T).dot(grad_a1)\n",
    "    ####################################################################################################\n",
    "    \n",
    "    \n",
    "    ################################ Update Weights Block using SGD ####################################\n",
    "    W3 -= learning_rate * grad_W3\n",
    "    W2 -= learning_rate * grad_W2\n",
    "    W1 -= learning_rate * grad_W1\n",
    "    ####################################################################################################\n",
    "    \n",
    "#plotting the loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(loss_list)\n",
    "plt.title('Loss vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Loading the test data from data/X_test.npy and data/y_test.npy.'''\n",
    "x_test = np.load('./data/X_test.npy')\n",
    "x_test = x_test.flatten().reshape(-1,28*28)\n",
    "x_test = x_test / 255.0\n",
    "y_test = np.load('./data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy is 67.31 %\n"
     ]
    }
   ],
   "source": [
    "batch_size_test = 100 # Deliberately taken 100 so that it divides the test data size\n",
    "num_minibatches = len(y_test)/batch_size_test\n",
    "test_correct = 0\n",
    "\n",
    "'''Only forward block code and compute softmax_score .'''\n",
    "for i_iter in range(int(num_minibatches)):\n",
    "    \n",
    "    '''Get one minibatch'''\n",
    "    batch_elem_idx = i_iter%num_minibatches\n",
    "    x_batchinput = x_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    ######### copy only the forward pass block of your code and pass the x_batchinput to it and compute softmax_score ##########\n",
    "    a1 =np.dot(x_batchinput,W1) # (64*784)*(784 * 512)\n",
    "    # implement Relu layer\n",
    "    h1 = np.where(a1>0,a1,0) # 512*64\n",
    "    #  implement 2 hidden layer\n",
    "    a2 = np.dot(h1,W2) #64*512 512*256# (512*256)' * 512*64 \n",
    "    # implement Relu activation \n",
    "    h2 = np.where(a2>0,a2,0) # 256*64\n",
    "    #implement linear output layer\n",
    "    a3 = np.dot(h2,W3) #64*256 256*10#(256*10)' * 256*64\n",
    "    # softmax layer\n",
    "    softmax_score = softmax(a3) # 64*10 #10*64\n",
    "    ##################################################################################\n",
    "    \n",
    "    y_batchinput = y_test[i_iter*batch_size_test:(i_iter+1)*batch_size_test]\n",
    "    \n",
    "    y_pred = np.argmax(softmax_score, axis=1)\n",
    "    num_correct_i_iter = np.sum(y_pred == y_batchinput)\n",
    "    test_correct += num_correct_i_iter\n",
    "print (\"Test accuracy is {:4.2f} %\".format(test_correct/len(y_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2_Hidden_MLP_New.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
